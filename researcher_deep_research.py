"""
Deep Research ç‰ˆæœ¬çš„ AI Daily News Brief æ ¸å¿ƒç ”ç©¶å¼•æ“
åŸºäº Gemini Deep Research Agent (deep-research-pro-preview-12-2025)
"""

import os
import json
import re
import time
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import markdown
from google import genai
from google.genai import types
from notion_client import Client
from github import Github

# è®¾ç½®åŒ—äº¬æ—¶åŒº
TZ_CN = ZoneInfo("Asia/Shanghai")

# åŸºç¡€é…ç½®
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
DATABASE_ID = os.getenv("NOTION_DATABASE_ID")

# GitHub é…ç½®
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
GITHUB_REPO = os.getenv("GITHUB_REPOSITORY")

# é‚®ç®±é…ç½® (SMTP)
EMAIL_HOST = os.getenv("EMAIL_HOST", "smtp.gmail.com")
EMAIL_PORT = int(os.getenv("EMAIL_PORT", 587))
EMAIL_USER = os.getenv("EMAIL_USER")
EMAIL_PASSWORD = os.getenv("EMAIL_PASSWORD")
NOTION_SUBSCRIBERS_DB_ID = os.getenv("NOTION_SUBSCRIBERS_DB_ID")
TEST_RECIPIENT = os.getenv("TEST_RECIPIENT")

client = genai.Client(api_key=GEMINI_API_KEY)
notion = Client(auth=NOTION_TOKEN) if NOTION_TOKEN else None


def run_deep_research():
    """
    ä½¿ç”¨ Gemini Deep Research Agent è¿›è¡Œæ·±åº¦ç ”ç©¶
    è¿”å›: åŸå§‹ç ”ç©¶æŠ¥å‘Šæ–‡æœ¬
    """
    print("ğŸ”¬ æ­£åœ¨å¯åŠ¨ Deep Research Agent...")
    print("â³ é¢„è®¡è€—æ—¶ 5-15 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    current_date = datetime.now(TZ_CN).strftime('%Y-%m-%d')
    yesterday = (datetime.now(TZ_CN) - timedelta(days=1)).strftime('%Y-%m-%d')
    
    # ä¸“ä¸º Deep Research è®¾è®¡çš„ç ”ç©¶ä»»åŠ¡æè¿°
    research_task = f"""
# AI Industry Intelligence Report - {current_date}

## Research Objective
You are a senior AI industry analyst tasked with creating a comprehensive daily intelligence report for AI professionals and enthusiasts. Your goal is to identify and analyze the most significant AI-related developments from the past 24 hours ({yesterday} to {current_date}).

## Research Scope

### Time Range
- Focus STRICTLY on events, news, and discussions from {yesterday} to today ({current_date})
- Verify publication dates to ensure freshness

### Coverage Areas (Equal Priority)
1. **Major Product Launches & Announcements**
   - OpenAI, Google (DeepMind), Anthropic, Meta, NVIDIA, Microsoft
   - Official blog posts, product releases, model updates
   - Sources: company blogs, press releases

2. **Academic & Technical Breakthroughs**
   - ArXiv papers with significant impact
   - HuggingFace trending models and papers
   - Novel techniques, algorithms, or architectures
   - Sources: arxiv.org, huggingface.co/papers

3. **Open Source & Developer Tools**
   - GitHub trending repositories (AI/ML category)
   - Developer tools, libraries, frameworks
   - Community-built applications
   - Sources: github.com/trending, reddit.com/r/LocalLlama, reddit.com/r/MachineLearning

4. **Industry & Business Developments**
   - Funding announcements, acquisitions, partnerships
   - Talent movement (key hires, departures)
   - Market analysis and industry trends
   - Sources: TechCrunch, VentureBeat, The Information

5. **Community Discussions & Sentiment**
   - Hot topics on Reddit (r/LocalLlama, r/MachineLearning)
   - Hacker News discussions
   - Twitter/X trending AI topics
   - Sources: news.ycombinator.com, reddit.com, twitter.com

## Search Strategy Recommendations

### Suggested Query Patterns
- `site:openai.com/blog OR site:anthropic.com OR site:deepmind.google after:YYYY-MM-DD`
- `"AI news" OR "artificial intelligence" after:{yesterday}`
- `site:techcrunch.com/category/artificial-intelligence after:{yesterday}`
- `site:arxiv.org (machine learning OR deep learning) after:{yesterday}`
- `site:github.com/trending python AI after:{yesterday}`
- `site:reddit.com/r/LocalLlama OR site:reddit.com/r/MachineLearning`
- `site:news.ycombinator.com (AI OR GPT OR LLM)`

### Source Diversity
- Aim for at least 15-20 unique sources
- Balance between official announcements and community discussions
- Verify information from multiple sources when possible

## Output Requirements

### Language
- Write the entire report in **Simplified Chinese (ç®€ä½“ä¸­æ–‡)**

### Structure
The report must follow this EXACT format:

---START_METADATA---
{{
  "title": "ä»Šæ—¥æœ€éœ‡æ’¼çš„å¤´æ¡æ ‡é¢˜ï¼ˆä¸è¶…è¿‡30å­—ï¼Œå¿…é¡»åŸºäºçœŸå®äº‹ä»¶ï¼‰",
  "summary": "60-100å­—çš„ç²¾å‡†æ‘˜è¦ï¼ŒåŒ…å«3-4ä¸ªæ ¸å¿ƒè¦ç‚¹ï¼Œç”¨åˆ†å·åˆ†éš”",
  "tags": ["æ ¸å¿ƒæŠ€æœ¯æ ‡ç­¾1", "æ ¸å¿ƒæŠ€æœ¯æ ‡ç­¾2", "è¡Œä¸šæ ‡ç­¾"],
  "importance": 8
}}
---END_METADATA---

---START_CONTENT---
# ğŸ’¡ é¦–å¸­æ´å¯Ÿ (Chief Insight)

ï¼ˆç”¨150-200å­—ç»¼åˆåˆ†æä»Šæ—¥AIè¡Œä¸šçš„æ•´ä½“å±€åŠ¿ã€‚å¿…é¡»åŸºäºå®é™…å‘ç”Ÿçš„äº‹ä»¶ï¼ŒæŒ‡å‡ºè¶‹åŠ¿ã€å…³è”æ€§å’Œæ½œåœ¨å½±å“ã€‚é¿å…ç©ºæ³›è¯„è®ºã€‚ï¼‰

## ğŸ”¥ æ ¸å¿ƒæƒ…æŠ¥

### 1. [å…·ä½“ä¸”å¸å¼•äººçš„æ ‡é¢˜]
**æ¥æº**: [åª’ä½“/æœºæ„åç§°](å®Œæ•´URL) | å‘å¸ƒæ—¶é—´: YYYY-MM-DD HH:MM

- **æ·±åº¦æ‹†è§£**: ï¼ˆ80-120å­—ï¼Œè§£é‡Šæ ¸å¿ƒæŠ€æœ¯ã€äº§å“åŠŸèƒ½ã€æˆ–äº‹ä»¶èƒŒæ™¯ã€‚é¿å…å¤è¿°æ–°é—»ï¼Œè¦æä¾›æ´å¯Ÿã€‚ï¼‰

- **ä¸ºä½•é‡è¦**: ï¼ˆ50-80å­—ï¼Œåˆ†æçŸ­æœŸå’Œé•¿æœŸå½±å“ã€‚å¯¹è¡Œä¸šã€å¼€å‘è€…ã€æˆ–ç”¨æˆ·æ„å‘³ç€ä»€ä¹ˆï¼Ÿï¼‰

- **ç¤¾åŒºå£°éŸ³**: ï¼ˆ40-60å­—ï¼ŒRedditã€Hacker Newsã€Twitterä¸Šçš„å…³é”®è®¨è®ºç‚¹æˆ–äº‰è®®ã€‚å¦‚æ— ç¤¾åŒºè®¨è®ºåˆ™è¯´æ˜"ç¤¾åŒºå°šæœªå¹¿æ³›è®¨è®º"ã€‚ï¼‰

### 2. [å¦ä¸€æ¡é‡è¦æƒ…æŠ¥çš„æ ‡é¢˜]
...ï¼ˆé‡å¤ä¸Šè¿°ç»“æ„ï¼‰

### 3. [ç¬¬ä¸‰æ¡æƒ…æŠ¥]
...

### 4. [ç¬¬å››æ¡æƒ…æŠ¥]
...

ï¼ˆç»§ç»­æ·»åŠ ï¼Œç¡®ä¿æ€»å…±æœ‰ 4-6 æ¡ç‹¬ç«‹çš„æ ¸å¿ƒæƒ…æŠ¥ï¼‰

## ğŸ› ï¸ æå®¢æ¨è (GitHub/Tools)

- **[é¡¹ç›®åç§°](GitHub URL)**: ï¼ˆ50-80å­—ä»‹ç»ã€‚åŒ…å«ï¼šæ ¸å¿ƒåŠŸèƒ½ã€æŠ€æœ¯æ ˆã€ä¸ºä½•å€¼å¾—å…³æ³¨ï¼ˆå¦‚Starå¢é•¿ã€è§£å†³çš„ç—›ç‚¹ã€ç‹¬ç‰¹ä¼˜åŠ¿ç­‰ï¼‰ã€‚å¦‚æœ‰å…·ä½“æ•°æ®æ›´ä½³ï¼Œå¦‚"24å°æ—¶å†…è·å¾—2000+ Stars"ã€‚ï¼‰

- **[ç¬¬äºŒä¸ªé¡¹ç›®åç§°](GitHub URL)**: ...

ï¼ˆè‡³å°‘2ä¸ªï¼Œæœ€å¤š4ä¸ªé«˜è´¨é‡å¼€æºé¡¹ç›®ï¼‰

## ğŸ”— åŸå§‹æƒ…æŠ¥æ¥æº

- [æ¥æºæ ‡é¢˜1 - æœºæ„åç§°](å®Œæ•´URL)
- [æ¥æºæ ‡é¢˜2 - æœºæ„åç§°](å®Œæ•´URL)
- [æ¥æºæ ‡é¢˜3 - æœºæ„åç§°](å®Œæ•´URL)
...

ï¼ˆåˆ—å‡ºè‡³å°‘10ä¸ªä¸»è¦å‚è€ƒæ¥æºï¼ŒåŒ…å«æ‰€æœ‰å¼•ç”¨çš„URLï¼‰

---END_CONTENT---

### Quality Standards
1. **Accuracy**: Every fact must be verifiable with a valid URL
2. **Freshness**: All events must be from the past 24 hours
3. **Depth**: Don't just summarize headlines - provide analysis and context
4. **Relevance**: Focus on high-impact news, not minor updates
5. **Completeness**: Must have 4-6 core intelligence items covering different aspects
6. **Citation**: Every claim should be backed by a source URL with publication time

### What to Avoid
- âŒ Generic commentary without specific events
- âŒ Outdated news (older than 24 hours)
- âŒ Speculation without evidence
- âŒ Repeating the same information multiple times
- âŒ Including only 1-2 news items (must have 4-6)

## Target Audience
AI professionals, developers, researchers, and enthusiasts who need:
- High signal-to-noise ratio
- Deep technical understanding
- Actionable insights
- Time savings (they don't want to browse 20 sites daily)

## Your Mission
Act as their personal AI intelligence officer. Spend the necessary time to thoroughly research, verify, and synthesize the most important AI developments of the day. Quality over speed.
    """
    
    try:
        start_time = time.time()
        
        # åˆ›å»ºåå°ç ”ç©¶ä»»åŠ¡
        interaction = client.interactions.create(
            input=research_task,
            agent='deep-research-pro-preview-12-2025',
            background=True  # å¼‚æ­¥æ‰§è¡Œï¼Œå› ä¸ºå¯èƒ½éœ€è¦ 5-20 åˆ†é’Ÿ
        )
        
        print(f"âœ… ç ”ç©¶ä»»åŠ¡å·²å¯åŠ¨: {interaction.id}")
        print("ğŸ“Š ä»»åŠ¡çŠ¶æ€ç›‘æ§ä¸­...")
        
        # è½®è¯¢æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
        poll_count = 0
        while True:
            poll_count += 1
            interaction = client.interactions.get(interaction.id)
            
            status = interaction.status
            
            if status == "completed":
                elapsed = time.time() - start_time
                print(f"âœ… ç ”ç©¶å®Œæˆï¼è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
                
                # è·å–æœ€ç»ˆè¾“å‡º
                if interaction.outputs and len(interaction.outputs) > 0:
                    result = interaction.outputs[-1].text
                    print(f"ğŸ“ æŠ¥å‘Šé•¿åº¦: {len(result)} å­—ç¬¦")
                    return result
                else:
                    raise Exception("ç ”ç©¶å®Œæˆä½†æ— è¾“å‡ºå†…å®¹")
                    
            elif status == "failed":
                error_msg = getattr(interaction, 'error', 'æœªçŸ¥é”™è¯¯')
                raise Exception(f"ç ”ç©¶ä»»åŠ¡å¤±è´¥: {error_msg}")
                
            else:
                # æ­£åœ¨è¿›è¡Œä¸­
                elapsed = time.time() - start_time
                print(f"â³ [{poll_count}] çŠ¶æ€: {status} | å·²è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
                
                # æ¯ 30 ç§’æ£€æŸ¥ä¸€æ¬¡
                time.sleep(30)
                
                # è¶…æ—¶ä¿æŠ¤ï¼ˆæœ€å¤šç­‰å¾… 60 åˆ†é’Ÿï¼‰
                if elapsed > 3600:
                    raise Exception("ä»»åŠ¡è¶…æ—¶ï¼ˆè¶…è¿‡60åˆ†é’Ÿï¼‰")
                    
    except Exception as e:
        print(f"âŒ Deep Research æ‰§è¡Œå¤±è´¥: {e}")
        raise


def run_gemini3_research_fallback():
    """
    é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨åŸæœ‰çš„ generate_content æ–¹å¼
    å½“ Deep Research ä¸å¯ç”¨æˆ–å¤±è´¥æ—¶ä½¿ç”¨
    """
    print("ğŸ”„ ä½¿ç”¨é™çº§æ–¹æ¡ˆ: Gemini 3 Pro generate_content")
    
    current_date = datetime.now(TZ_CN).strftime('%Y-%m-%d')
    yesterday = (datetime.now(TZ_CN) - timedelta(days=1)).strftime('%Y-%m-%d')
    
    mode_instruction = "é‡ç‚¹å…³æ³¨ï¼šOpenAI, Google, Anthropic, è‹±ä¼Ÿè¾¾, Meta ç­‰å·¨å¤´çš„æœ€æ–°å‘å¸ƒå’Œæ–°é—»ã€‚ä»¥åŠ ArXiv ä¸Šçš„çªç ´æ€§è®ºæ–‡ã€‚"

    prompt = f"""
    # è§’è‰²å®šä¹‰
    ä½ æ˜¯ä¸€ä½ç«™åœ¨ AI è¡Œä¸šæœ€å‰æ²¿çš„ã€é¦–å¸­æƒ…æŠ¥å®˜ã€‘ï¼Œæ“…é•¿ä»æµ·é‡ç¢ç‰‡ä¿¡æ¯(åŒ…æ‹¬æ–°é—»ã€è®ºæ–‡ã€ç¤¾åŒºè®¨è®º)ä¸­æå–æœ€æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚
    
    # æ—¶æ•ˆä¸ç¯å¢ƒ
    ä»Šå¤©æ˜¯ {current_date}ã€‚
    ä½ çš„æœç´¢èŒƒå›´æ˜¯ {yesterday} è‡³ä»Šã€‚
    {mode_instruction}
    
    # ä½ çš„ç›®æ ‡
    ä¸ºè®¢é˜…è€…æä¾›ä¸€ä»½**"é«˜ä¿¡å™ªæ¯”ã€å¤šç»´åº¦"**çš„æƒ…æŠ¥ã€‚
    **ä¸è¦**åªåˆ—å‡º 1-2 æ¡æ–°é—»ï¼Œè¯·ç¡®ä¿æŠ¥å‘ŠåŒ…å« **4-6 ä¸ª** ç‹¬ç«‹ä¸”æœ‰æ·±åº¦çš„æƒ…æŠ¥ç‚¹ã€‚ä¼˜å…ˆçº§ï¼šçœŸå®æ€§ > æ–°é²œåº¦ > å®Œæ•´æ€§
    å¦‚æœå®˜æ–¹æ–°é—»å¾ˆå°‘ï¼Œè¯·æŒ–æ˜ç¤¾åŒº(Reddit/HN/Twitter)çš„çƒ­é—¨è®®é¢˜ã€‚

    # æ·±åº¦ç ”ç©¶ä»»åŠ¡ (å¿…é¡»è¦†ç›–ä»¥ä¸‹ç»´åº¦)
    1. **ï¸ğŸ”¥ å¤´æ¡èšç„¦**ï¼šè¿‡å» 24h å½±å“æœ€å¤§çš„äº‹ä»¶ (å¯ä»¥æ˜¯å‘å¸ƒã€ä¹Ÿå¯ä»¥æ˜¯äº‰è®®/è®¨è®º)ã€‚
    2. **ğŸ§ª å­¦æœ¯ä¸æŠ€æœ¯**ï¼šArXiv çƒ­é—¨è®ºæ–‡ æˆ– HuggingFace ä¸Šçš„æ–°æ™‹ SOTA æ¨¡å‹ã€‚
    3. **ğŸ› ï¸ å¼€æºä¸é»‘å®¢**ï¼šGitHub Trending æˆ– Reddit ä¸Šè¢«å¼€å‘è€…çƒ­è®®çš„å®æˆ˜å·¥å…·/Trickã€‚
    4. **ğŸ“‰ å•†ä¸šä¸é£å‘**ï¼šèèµ„ã€äººæ‰æµåŠ¨æˆ–è¡Œä¸šåˆ†æã€‚
    
    # æœç´¢ç­–ç•¥ (å¤šæ ·åŒ–)
    - å®˜æ–¹æºï¼š`site:openai.com/blog`, `site:anthropic.com`
    - èµ„è®¯æºï¼š`AI news after:{yesterday}`, `VentureBeat AI`, `TechCrunch AI`
    - ç¤¾åŒºæºï¼š`site:reddit.com/r/LocalLlama top 24h`, `site:news.ycombinator.com AI`, `site:huggingface.co/papers`
    
    # è¾“å‡ºè¦æ±‚ (Markdown + JSON)
    ä½ çš„è¾“å‡ºå¿…é¡»åŒ…å«ã€æ€è€ƒè¿‡ç¨‹ã€‘å¹¶ä¸¥æ ¼éµå®ˆä»¥ä¸‹æ ¼å¼ï¼š
    
    ---START_METADATA---
    {{
      "title": "ä»Šæ—¥æœ€æœ‰éœ‡æ’¼åŠ›çš„å¤´æ¡æ ‡é¢˜ï¼ˆä¸è¶…è¿‡ 30 å­—ï¼‰",
      "summary": "60-100 å­—çš„ç²¾å‡†æ‘˜è¦ï¼Œå¿…é¡»åŒ…å« 3-4 ä¸ªæ ¸å¿ƒè¦ç‚¹ï¼Œæ¯ä¸ªè¦ç‚¹ç”¨åˆ†å·åˆ†éš”",
      "tags": ["æ ¸å¿ƒæŠ€æœ¯æ ‡ç­¾","æ ¸å¿ƒæŠ€æœ¯æ ‡ç­¾2", "è¡Œä¸šæ ‡ç­¾"],
      "importance": 9
    }}
    ---END_METADATA---
    
    ---START_CONTENT---
    # ğŸ’¡ é¦–å¸­æ´å¯Ÿ (Chief Insight)
    (ç”¨ä¸€æ®µè¯åˆæˆä»Šæ—¥çš„æ•´ä½“å±€åŠ¿ã€‚å¿…é¡»åŸºäºçœŸå®å‘ç”Ÿçš„äº‹ä»¶ï¼Œé¿å…ç©ºæ³›è¯„è®º)
    
    ## ğŸ”¥ æ ¸å¿ƒæƒ…æŠ¥ (4-6æ¡)
    
    ### 1. [æƒ…æŠ¥æ ‡é¢˜]
    **æ¥æº**: [åª’ä½“/ç¤¾åŒºåç§°](URL) | å‘å¸ƒæ—¶é—´
    
    - **æ·±åº¦æ‹†è§£**: (50-100å­—ï¼Œæ ¸å¿ƒæŠ€æœ¯æˆ–äº‹ä»¶è„‰ç»œ)
    
    - **ä¸ºä½•é‡è¦**: (ä¸€å¥è¯ç‚¹å‡ºè¯´æ˜å¯¹è¡Œä¸šçš„çŸ­æœŸå’Œé•¿æœŸå½±å“ã€‚)
    
    - **ç¤¾åŒºå£°éŸ³**: (Redditã€Hacker Newsã€Twitter ä¸Šçš„å…³é”®äº‰è®®æˆ–å¥½è¯„)
    
    ### 2. [æƒ…æŠ¥æ ‡é¢˜]
    ...
    
    ### 3. [æƒ…æŠ¥æ ‡é¢˜]
    ...
    
    ## ğŸ› ï¸ æå®¢æ¨è (GitHub/Tools)
    - **[é¡¹ç›®å](URL)**: (ä¸€å¥è¯ä»‹ç»æ ¸å¿ƒåŠŸèƒ½ + ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ï¼ˆå¦‚ï¼šStar æ•°å¢é•¿ã€è§£å†³çš„ç—›ç‚¹ç­‰ï¼‰)
    
    ## ğŸ”— åŸå§‹æƒ…æŠ¥æ¥æº
    - [æ ‡é¢˜](URL)
    ---END_CONTENT---
    """

    response = client.models.generate_content(
        model='gemini-2.0-flash-exp',  # ä½¿ç”¨æ›´ç¨³å®šçš„æ¨¡å‹
        contents=prompt,
        config=types.GenerateContentConfig(
            tools=[types.Tool(google_search=types.GoogleSearch())],
        )
    )
    return response.text


def parse_gemini_response(raw_text):
    """ä»åŸå§‹æ–‡æœ¬ä¸­æå–å…ƒæ•°æ®å’Œæ­£æ–‡å†…å®¹"""
    try:
        metadata_match = re.search(r'---START_METADATA---(.*?)---END_METADATA---', raw_text, re.DOTALL)
        content_match = re.search(r'---START_CONTENT---(.*?)---END_CONTENT---', raw_text, re.DOTALL)
        
        if metadata_match and content_match:
            json_str = metadata_match.group(1).strip()
            # æ¸…ç† JSON ä¸­å¯èƒ½åŒ…è£¹çš„ markdown ä»£ç å—æ ‡è¯†
            json_str = re.sub(r'^```json|```$', '', json_str, flags=re.MULTILINE).strip()
            metadata = json.loads(json_str)
            content = content_match.group(1).strip()
            return metadata, content
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡è®°ï¼Œå°è¯•ç”¨ AI é‡æ–°æ ¼å¼åŒ–
            print("âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†æ ¼å¼æ ‡è®°ï¼Œå°è¯•ä½¿ç”¨ AI é‡æ–°æ ¼å¼åŒ–...")
            return reformat_with_ai(raw_text)
    except Exception as e:
        print(f"âš ï¸ è§£æå‡ºé”™: {e}")
        return None, raw_text


def reformat_with_ai(raw_report):
    """
    ä½¿ç”¨ Gemini å°†éæ ‡å‡†æ ¼å¼çš„æŠ¥å‘Šè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    è¿™æ˜¯ä¸€ä¸ªä¿é™©æªæ–½ï¼Œç¡®ä¿å³ä½¿ Deep Research è¾“å‡ºæ ¼å¼ä¸ç¬¦ä¹Ÿèƒ½å¤„ç†
    """
    print("ğŸ”„ æ­£åœ¨ä½¿ç”¨ AI é‡æ–°æ ¼å¼åŒ–æŠ¥å‘Š...")
    
    conversion_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªå†…å®¹æ ¼å¼åŒ–ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹ AI ç ”ç©¶æŠ¥å‘Šè½¬æ¢ä¸ºæŒ‡å®šçš„æ ‡å‡†æ ¼å¼ã€‚

è¦æ±‚ï¼š
1. æå–æ‰€æœ‰å…³é”®ä¿¡æ¯
2. ä¿ç•™æ‰€æœ‰æ¥æº URL å’Œæ—¶é—´
3. ä½¿ç”¨ç®€ä½“ä¸­æ–‡
4. ä¸¥æ ¼éµå®ˆè¾“å‡ºæ ¼å¼

è¾“å‡ºæ ¼å¼ï¼š
---START_METADATA---
{{
  "title": "æœ€éœ‡æ’¼çš„å¤´æ¡æ ‡é¢˜ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
  "summary": "60-100å­—çš„æ‘˜è¦ï¼ŒåŒ…å«3-4ä¸ªè¦ç‚¹ï¼Œç”¨åˆ†å·åˆ†éš”",
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"],
  "importance": 8
}}
---END_METADATA---

---START_CONTENT---
# ğŸ’¡ é¦–å¸­æ´å¯Ÿ
ï¼ˆç»¼åˆåˆ†ææ®µè½ï¼‰

## ğŸ”¥ æ ¸å¿ƒæƒ…æŠ¥

### 1. [æ ‡é¢˜]
**æ¥æº**: [åç§°](URL) | æ—¶é—´

- **æ·±åº¦æ‹†è§£**: ...
- **ä¸ºä½•é‡è¦**: ...
- **ç¤¾åŒºå£°éŸ³**: ...

ï¼ˆç»§ç»­å…¶ä»–æƒ…æŠ¥é¡¹...ï¼‰

## ğŸ› ï¸ æå®¢æ¨è
- **[é¡¹ç›®](URL)**: ...

## ğŸ”— åŸå§‹æƒ…æŠ¥æ¥æº
- [æ ‡é¢˜](URL)
---END_CONTENT---

åŸå§‹æŠ¥å‘Šï¼š
{raw_report}
"""
    
    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp',
            contents=conversion_prompt
        )
        
        formatted_text = response.text
        return parse_gemini_response(formatted_text)
        
    except Exception as e:
        print(f"âŒ AI é‡æ–°æ ¼å¼åŒ–å¤±è´¥: {e}")
        # è¿”å›åŸºæœ¬çš„å…ƒæ•°æ®å’ŒåŸå§‹å†…å®¹
        default_meta = {
            "title": f"AI æ·±åº¦ç®€æŠ¥ - {datetime.now(TZ_CN).strftime('%Y-%m-%d')}",
            "summary": "ä»Šæ—¥ AI è¡Œä¸šæƒ…æŠ¥å·²é€è¾¾",
            "tags": ["AI", "æ·±åº¦ç ”ç©¶"],
            "importance": 7
        }
        return default_meta, raw_report


# ä»¥ä¸‹å‡½æ•°ä¸åŸç‰ˆ researcher.py ç›¸åŒï¼Œç›´æ¥å¤ç”¨
# åŒ…æ‹¬: split_content_to_blocks, save_to_notion, update_archive_index, 
# update_homepage, save_to_markdown_file, publish_to_github_issue, send_email_newsletter

# [æ­¤å¤„çœç•¥è¿™äº›å‡½æ•°çš„ä»£ç ï¼Œåœ¨å®é™…ä½¿ç”¨æ—¶ä»åŸæ–‡ä»¶å¤åˆ¶è¿‡æ¥]


if __name__ == "__main__":
    if not GEMINI_API_KEY:
        print("âŒ é”™è¯¯: GEMINI_API_KEY æœªè®¾ç½®")
        exit(1)

    print("="*60)
    print("ğŸ¤– AI Daily News Brief - Deep Research Edition")
    print("="*60)
    
    # 1. è¿è¡Œ Deep Researchï¼ˆå¸¦é™çº§æœºåˆ¶ï¼‰
    try:
        raw_report = run_deep_research()
        print("\nâœ… Deep Research æ‰§è¡ŒæˆåŠŸ")
    except Exception as e:
        print(f"\nâš ï¸ Deep Research å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ: {e}")
        try:
            raw_report = run_gemini3_research_fallback()
            print("\nâœ… é™çº§æ–¹æ¡ˆæ‰§è¡ŒæˆåŠŸ")
        except Exception as e2:
            print(f"\nâŒ é™çº§æ–¹æ¡ˆä¹Ÿå¤±è´¥äº†: {e2}")
            exit(1)
    
    # 2. è§£æå†…å®¹ï¼ˆè‡ªåŠ¨å¤„ç†æ ¼å¼è½¬æ¢ï¼‰
    meta, body = parse_gemini_response(raw_report)
    if not meta:
        meta = {
            "title": f"AI æ·±åº¦ç®€æŠ¥ - {datetime.now(TZ_CN).strftime('%Y-%m-%d')}",
            "summary": "ä»Šæ—¥æƒ…æŠ¥å·²é€è¾¾",
            "tags": ["AI", "æ¯æ—¥ç®€æŠ¥"]
        }
    
    print(f"\nğŸ“‹ æŠ¥å‘Šæ ‡é¢˜: {meta.get('title')}")
    print(f"ğŸ“Š æŠ¥å‘Šé•¿åº¦: {len(body)} å­—ç¬¦")
    
    # 3-8. åç»­æµç¨‹ä¸åŸç‰ˆç›¸åŒ
    # [æ­¤å¤„éœ€è¦å¤åˆ¶åŸ researcher.py çš„æ­¥éª¤ 3-8]
    
    print("\n" + "="*60)
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("="*60)
