import os
import json
import re
import time
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import markdown
from google import genai
from google.genai import types
from notion_client import Client
from github import Github

# è®¾ç½®åŒ—äº¬æ—¶åŒº
TZ_CN = ZoneInfo("Asia/Shanghai")

# 1. åŸºç¡€é…ç½®
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
DATABASE_ID = os.getenv("NOTION_DATABASE_ID")

# GitHub é…ç½®
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
GITHUB_REPO = os.getenv("GITHUB_REPOSITORY")  # æ ¼å¼: "username/repo"

# é‚®ç®±é…ç½® (SMTP)
EMAIL_HOST = os.getenv("EMAIL_HOST", "smtp.gmail.com")
EMAIL_PORT = int(os.getenv("EMAIL_PORT", 587))
EMAIL_USER = os.getenv("EMAIL_USER")
EMAIL_PASSWORD = os.getenv("EMAIL_PASSWORD")
NOTION_SUBSCRIBERS_DB_ID = os.getenv("NOTION_SUBSCRIBERS_DB_ID")
TEST_RECIPIENT = os.getenv("TEST_RECIPIENT")

client = genai.Client(api_key=GEMINI_API_KEY)
notion = Client(auth=NOTION_TOKEN)

def send_email_newsletter(metadata, markdown_content):
    """é€šè¿‡ SMTP å‘é€ HTML æ ¼å¼çš„ç®€æŠ¥é‚®ä»¶"""
    if not EMAIL_USER or not EMAIL_PASSWORD:
        return

    print("ğŸ“§ æ­£åœ¨å¯åŠ¨ SMTP é‚®ä»¶æ¨é€...")
    
    recipients = []
    if TEST_RECIPIENT:
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: ä»…å‘é€ç»™ {TEST_RECIPIENT}")
        recipients.append(TEST_RECIPIENT)
    elif NOTION_SUBSCRIBERS_DB_ID and notion:
        print("ğŸ‘¥ æ­£åœ¨ä» Notion è¯»å–è®¢é˜…è€…åˆ—è¡¨...")
        try:
            # æŸ¥è¯¢ Notion æ•°æ®åº“ (åˆ†é¡µè·å–æ‰€æœ‰ç”¨æˆ·)
            has_more = True
            start_cursor = None
            
            while has_more:
                # ä½¿ç”¨ Notion API 2025 (Data Sources)
                query_kwargs = {
                    "data_source_id": NOTION_SUBSCRIBERS_DB_ID,
                    "page_size": 100
                }
                if start_cursor:
                    query_kwargs["start_cursor"] = start_cursor
                
                resp = notion.data_sources.query(**query_kwargs)
                
                for page in resp.get("results", []):
                    props = page.get("properties", {})
                    email = ""
                    # å°è¯•å¯»æ‰¾å¸¸è§çš„é‚®ç®±åˆ—å (Email, é‚®ç®±, Mail)
                    for key, val in props.items():
                        if "mail" in key.lower() or "é‚®ç®±" in key:
                            # æ ¹æ® Notion å­—æ®µç±»å‹æå–æ–‡æœ¬
                            if val["type"] == "email":
                                email = val["email"]
                            elif val["type"] == "rich_text" and val["rich_text"]:
                                email = val["rich_text"][0]["text"]["content"]
                            elif val["type"] == "title" and val["title"]:
                                email = val["title"][0]["text"]["content"]
                            break
                    
                    if email and "@" in email:
                        recipients.append(email.strip())
                
                has_more = resp.get("has_more")
                start_cursor = resp.get("next_cursor")
                
            # å»é‡
            recipients = list(set(recipients))
            print(f"ğŸ‘¥ å…±è·å–åˆ° {len(recipients)} ä½è®¢é˜…è€…")
            
        except Exception as e:
            print(f"âŒ è¯»å– Notion è®¢é˜…åˆ—è¡¨å¤±è´¥: {e}")
    
    if not recipients:
        print("âš ï¸ æ²¡æœ‰æ”¶ä»¶äºº (è¯·é…ç½® TEST_RECIPIENT æˆ– æ£€æŸ¥ Notion è¿æ¥)ï¼Œè·³è¿‡å‘é€")
        return

    # å°† Markdown è½¬æ¢ä¸º HTML
    html_body = markdown.markdown(markdown_content)
    
    full_html = f"""
    <html>
    <head>
        <style>
            body {{ font-family: sans-serif; line-height: 1.6; color: #333; max-width: 800px; margin: 0 auto; padding: 20px; }}
            h1 {{ color: #007bff; border-bottom: 2px solid #007bff; padding-bottom: 10px; }}
            h2 {{ color: #333; margin-top: 30px; }}
            a {{ color: #007bff; text-decoration: none; }}
            blockquote {{ border-left: 4px solid #007bff; margin: 0; padding-left: 15px; color: #555; background: #f9f9f9; padding: 10px; }}
        </style>
    </head>
    <body>
        <div style="text-align: center; margin-bottom: 20px;">
            <p>ğŸ‘‡ ç‚¹å‡»ä¸‹æ–¹é“¾æ¥æŸ¥çœ‹å®Œæ•´æ’ç‰ˆ ğŸ‘‡</p>
            <a href="{os.getenv('mkdocs_site_url', 'https://news.helloaidev.com')}" style="background: #007bff; color: white; padding: 10px 20px; border-radius: 5px; text-decoration: none;">åœ¨æµè§ˆå™¨ä¸­é˜…è¯»</a>
        </div>
        {html_body}
        <div style="margin-top: 40px; font-size: 12px; color: #888; text-align: center;">
            <p>æœ¬é‚®ä»¶ç”± AI Daily News Brief è‡ªåŠ¨å‘é€</p>
        </div>
    </body>
    </html>
    """

    try:
        server = smtplib.SMTP(EMAIL_HOST, EMAIL_PORT)
        server.starttls()
        server.login(EMAIL_USER, EMAIL_PASSWORD)
        
        for email_addr in recipients:
            msg = MIMEMultipart()
            msg['From'] = f"AI Daily Brief <{EMAIL_USER}>"
            msg['To'] = email_addr
            date_str = datetime.now(TZ_CN).strftime('%Y-%m-%d')
            msg['Subject'] = f"ğŸ¤– {metadata.get('title')} ({date_str})"
            msg.attach(MIMEText(full_html, 'html'))
            server.send_message(msg)
            print(f"âœ… é‚®ä»¶å·²å‘é€è‡³: {email_addr}")
            
        server.quit()
    except Exception as e:
        print(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")

def run_deep_research():
    """
    ä½¿ç”¨ Gemini Deep Research Agent è¿›è¡Œæ·±åº¦ç ”ç©¶
    è¿”å›: åŸå§‹ç ”ç©¶æŠ¥å‘Šæ–‡æœ¬
    """
    print("ğŸ”¬ æ­£åœ¨å¯åŠ¨ Deep Research Agent...")
    print("â³ é¢„è®¡è€—æ—¶ 5-15 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    current_date = datetime.now(TZ_CN).strftime('%Y-%m-%d')
    yesterday = (datetime.now(TZ_CN) - timedelta(days=1)).strftime('%Y-%m-%d')
    
    # ä¸“ä¸º Deep Research è®¾è®¡çš„ç ”ç©¶ä»»åŠ¡æè¿°
    research_task = f"""
# AI Industry Intelligence Report - {current_date}

## Research Objective
You are a senior AI industry analyst tasked with creating a comprehensive daily intelligence report for AI professionals and enthusiasts. Your goal is to identify and analyze the most significant AI-related developments from the past 24 hours ({yesterday} to {current_date}).

## Research Scope

### Time Range
- Focus STRICTLY on events, news, and discussions from {yesterday} to today ({current_date})
- Verify publication dates to ensure freshness

### Coverage Areas (Equal Priority)
1. **Major Product Launches & Announcements**
   - OpenAI, Google (DeepMind), Anthropic, Meta, NVIDIA, Microsoft
   - Official blog posts, product releases, model updates

2. **Academic & Technical Breakthroughs**
   - ArXiv papers with significant impact
   - HuggingFace trending models and papers

3. **Open Source & Developer Tools**
   - GitHub trending repositories (AI/ML category)
   - Developer tools, libraries, frameworks

4. **Industry & Business Developments**
   - Funding announcements, acquisitions, partnerships
   - Market analysis and industry trends

5. **Community Discussions & Sentiment**
   - Hot topics on Reddit (r/LocalLlama, r/MachineLearning)
   - Hacker News discussions

## Output Requirements

### Language
- Write the entire report in **Simplified Chinese (ç®€ä½“ä¸­æ–‡)**

### Structure
The report must follow this EXACT format:

---START_METADATA---
{{
  "title": "ä»Šæ—¥æœ€éœ‡æ’¼çš„å¤´æ¡æ ‡é¢˜ï¼ˆä¸è¶…è¿‡30å­—ï¼Œå¿…é¡»åŸºäºçœŸå®äº‹ä»¶ï¼‰",
  "summary": "60-100å­—çš„ç²¾å‡†æ‘˜è¦ï¼ŒåŒ…å«3-4ä¸ªæ ¸å¿ƒè¦ç‚¹ï¼Œç”¨åˆ†å·åˆ†éš”",
  "tags": ["æ ¸å¿ƒæŠ€æœ¯æ ‡ç­¾1", "æ ¸å¿ƒæŠ€æœ¯æ ‡ç­¾2", "è¡Œä¸šæ ‡ç­¾"],
  "importance": 8
}}
---END_METADATA---

---START_CONTENT---
# ğŸ’¡ é¦–å¸­æ´å¯Ÿ (Chief Insight)

ï¼ˆç”¨150-200å­—ç»¼åˆåˆ†æä»Šæ—¥AIè¡Œä¸šçš„æ•´ä½“å±€åŠ¿ã€‚å¿…é¡»åŸºäºå®é™…å‘ç”Ÿçš„äº‹ä»¶ï¼ŒæŒ‡å‡ºè¶‹åŠ¿ã€å…³è”æ€§å’Œæ½œåœ¨å½±å“ã€‚é¿å…ç©ºæ³›è¯„è®ºã€‚ï¼‰

## ğŸ”¥ æ ¸å¿ƒæƒ…æŠ¥

### 1. [å…·ä½“ä¸”å¸å¼•äººçš„æ ‡é¢˜]
**æ¥æº**: [åª’ä½“/æœºæ„åç§°](å®Œæ•´URL) | å‘å¸ƒæ—¶é—´: YYYY-MM-DD

- **æ·±åº¦æ‹†è§£**: ï¼ˆ80-120å­—ï¼Œè§£é‡Šæ ¸å¿ƒæŠ€æœ¯ã€äº§å“åŠŸèƒ½ã€æˆ–äº‹ä»¶èƒŒæ™¯ã€‚é¿å…å¤è¿°æ–°é—»ï¼Œè¦æä¾›æ´å¯Ÿã€‚ï¼‰

- **ä¸ºä½•é‡è¦**: ï¼ˆ50-80å­—ï¼Œåˆ†æçŸ­æœŸå’Œé•¿æœŸå½±å“ã€‚ï¼‰

- **ç¤¾åŒºå£°éŸ³**: ï¼ˆ40-60å­—ï¼ŒRedditã€Hacker Newsã€Twitterä¸Šçš„å…³é”®è®¨è®ºç‚¹ã€‚ï¼‰

### 2. [å¦ä¸€æ¡é‡è¦æƒ…æŠ¥çš„æ ‡é¢˜]
...

ï¼ˆç»§ç»­æ·»åŠ ï¼Œç¡®ä¿æ€»å…±æœ‰ 4-6 æ¡ç‹¬ç«‹çš„æ ¸å¿ƒæƒ…æŠ¥ï¼‰

## ğŸ› ï¸ æå®¢æ¨è (GitHub/Tools)

- **[é¡¹ç›®åç§°](GitHub URL)**: ï¼ˆ50-80å­—ä»‹ç»æ ¸å¿ƒåŠŸèƒ½å’Œä¸ºä½•å€¼å¾—å…³æ³¨ï¼‰

ï¼ˆè‡³å°‘2ä¸ªï¼Œæœ€å¤š4ä¸ªé«˜è´¨é‡å¼€æºé¡¹ç›®ï¼‰

## ğŸ”— åŸå§‹æƒ…æŠ¥æ¥æº

- [æ¥æºæ ‡é¢˜1 - æœºæ„åç§°](å®Œæ•´URL)
...

ï¼ˆåˆ—å‡ºè‡³å°‘10ä¸ªä¸»è¦å‚è€ƒæ¥æºï¼‰

---END_CONTENT---

### Quality Standards
1. **Accuracy**: Every fact must be verifiable with a valid URL
2. **Freshness**: All events must be from the past 24 hours
3. **Depth**: Don't just summarize headlines - provide analysis and context
4. **Completeness**: Must have 4-6 core intelligence items covering different aspects

## Your Mission
Act as their personal AI intelligence officer. Spend the necessary time to thoroughly research, verify, and synthesize the most important AI developments of the day. Quality over speed.
    """
    
    try:
        start_time = time.time()
        
        # åˆ›å»ºåå°ç ”ç©¶ä»»åŠ¡
        interaction = client.interactions.create(
            input=research_task,
            agent='deep-research-pro-preview-12-2025',
            background=True
        )
        
        print(f"âœ… ç ”ç©¶ä»»åŠ¡å·²å¯åŠ¨: {interaction.id}")
        print("ğŸ“Š ä»»åŠ¡çŠ¶æ€ç›‘æ§ä¸­...")
        
        # è½®è¯¢æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
        poll_count = 0
        while True:
            poll_count += 1
            interaction = client.interactions.get(interaction.id)
            
            status = interaction.status
            
            if status == "completed":
                elapsed = time.time() - start_time
                print(f"âœ… ç ”ç©¶å®Œæˆï¼è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
                
                if interaction.outputs and len(interaction.outputs) > 0:
                    result = interaction.outputs[-1].text
                    print(f"ğŸ“ æŠ¥å‘Šé•¿åº¦: {len(result)} å­—ç¬¦")
                    return result
                else:
                    raise Exception("ç ”ç©¶å®Œæˆä½†æ— è¾“å‡ºå†…å®¹")
                    
            elif status == "failed":
                error_msg = getattr(interaction, 'error', 'æœªçŸ¥é”™è¯¯')
                raise Exception(f"ç ”ç©¶ä»»åŠ¡å¤±è´¥: {error_msg}")
                
            else:
                elapsed = time.time() - start_time
                print(f"â³ [{poll_count}] çŠ¶æ€: {status} | å·²è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
                time.sleep(30)
                
                if elapsed > 3600:
                    raise Exception("ä»»åŠ¡è¶…æ—¶ï¼ˆè¶…è¿‡60åˆ†é’Ÿï¼‰")
                    
    except Exception as e:
        print(f"âŒ Deep Research æ‰§è¡Œå¤±è´¥: {e}")
        raise


def run_gemini3_research_fallback():
    """
    é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨åŸæœ‰çš„ generate_content æ–¹å¼
    å½“ Deep Research ä¸å¯ç”¨æˆ–å¤±è´¥æ—¶ä½¿ç”¨
    """
    print("ğŸ”„ ä½¿ç”¨é™çº§æ–¹æ¡ˆ: Gemini generate_content")
    
    current_date = datetime.now(TZ_CN).strftime('%Y-%m-%d')
    yesterday = (datetime.now(TZ_CN) - timedelta(days=1)).strftime('%Y-%m-%d')
    
    mode_instruction = "é‡ç‚¹å…³æ³¨ï¼šOpenAI, Google, Anthropic, è‹±ä¼Ÿè¾¾, Meta ç­‰å·¨å¤´çš„æœ€æ–°å‘å¸ƒå’Œæ–°é—»ã€‚ä»¥åŠ ArXiv ä¸Šçš„çªç ´æ€§è®ºæ–‡ã€‚"

    prompt = f"""
    # è§’è‰²å®šä¹‰
    ä½ æ˜¯ä¸€ä½ç«™åœ¨ AI è¡Œä¸šæœ€å‰æ²¿çš„ã€é¦–å¸­æƒ…æŠ¥å®˜ã€‘ï¼Œæ“…é•¿ä»æµ·é‡ç¢ç‰‡ä¿¡æ¯(åŒ…æ‹¬æ–°é—»ã€è®ºæ–‡ã€ç¤¾åŒºè®¨è®º)ä¸­æå–æœ€æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚
    
    # æ—¶æ•ˆä¸ç¯å¢ƒ
    ä»Šå¤©æ˜¯ {current_date}ã€‚
    ä½ çš„æœç´¢èŒƒå›´æ˜¯ {yesterday} è‡³ä»Šã€‚
    {mode_instruction}
    
    # ä½ çš„ç›®æ ‡
    ä¸ºè®¢é˜…è€…æä¾›ä¸€ä»½**"é«˜ä¿¡å™ªæ¯”ã€å¤šç»´åº¦"**çš„æƒ…æŠ¥ã€‚
    **ä¸è¦**åªåˆ—å‡º 1-2 æ¡æ–°é—»ï¼Œè¯·ç¡®ä¿æŠ¥å‘ŠåŒ…å« **4-6 ä¸ª** ç‹¬ç«‹ä¸”æœ‰æ·±åº¦çš„æƒ…æŠ¥ç‚¹ã€‚ä¼˜å…ˆçº§ï¼šçœŸå®æ€§ > æ–°é²œåº¦ > å®Œæ•´æ€§
    å¦‚æœå®˜æ–¹æ–°é—»å¾ˆå°‘ï¼Œè¯·æŒ–æ˜ç¤¾åŒº(Reddit/HN/Twitter)çš„çƒ­é—¨è®®é¢˜ã€‚

    # æ·±åº¦ç ”ç©¶ä»»åŠ¡ (å¿…é¡»è¦†ç›–ä»¥ä¸‹ç»´åº¦)
    1. **ï¸ğŸ”¥ å¤´æ¡èšç„¦**ï¼šè¿‡å» 24h å½±å“æœ€å¤§çš„äº‹ä»¶ (å¯ä»¥æ˜¯å‘å¸ƒã€ä¹Ÿå¯ä»¥æ˜¯äº‰è®®/è®¨è®º)ã€‚
    2. **ğŸ§ª å­¦æœ¯ä¸æŠ€æœ¯**ï¼šArXiv çƒ­é—¨è®ºæ–‡ æˆ– HuggingFace ä¸Šçš„æ–°æ™‹ SOTA æ¨¡å‹ã€‚
    3. **ğŸ› ï¸ å¼€æºä¸é»‘å®¢**ï¼šGitHub Trending æˆ– Reddit ä¸Šè¢«å¼€å‘è€…çƒ­è®®çš„å®æˆ˜å·¥å…·/Trickã€‚
    4. **ğŸ“‰ å•†ä¸šä¸é£å‘**ï¼šèèµ„ã€äººæ‰æµåŠ¨æˆ–è¡Œä¸šåˆ†æã€‚
    
    # æœç´¢ç­–ç•¥ (å¤šæ ·åŒ–)
    - å®˜æ–¹æºï¼š`site:openai.com/blog`, `site:anthropic.com`
    - èµ„è®¯æºï¼š`AI news after:{yesterday}`, `VentureBeat AI`, `TechCrunch AI`
    - ç¤¾åŒºæºï¼š`site:reddit.com/r/LocalLlama top 24h`, `site:news.ycombinator.com AI`, `site:huggingface.co/papers`
    
    # è¾“å‡ºè¦æ±‚ (Markdown + JSON)
    ä½ çš„è¾“å‡ºå¿…é¡»åŒ…å«ã€æ€è€ƒè¿‡ç¨‹ã€‘å¹¶ä¸¥æ ¼éµå®ˆä»¥ä¸‹æ ¼å¼ï¼š
    
    ---START_METADATA---
    {{
      "title": "ä»Šæ—¥æœ€æœ‰éœ‡æ’¼åŠ›çš„å¤´æ¡æ ‡é¢˜ï¼ˆä¸è¶…è¿‡ 30 å­—ï¼‰",
      "summary": "60-100 å­—çš„ç²¾å‡†æ‘˜è¦ï¼Œå¿…é¡»åŒ…å« 3-4 ä¸ªæ ¸å¿ƒè¦ç‚¹ï¼Œæ¯ä¸ªè¦ç‚¹ç”¨åˆ†å·åˆ†éš”",
      "tags": ["æ ¸å¿ƒæŠ€æœ¯æ ‡ç­¾","æ ¸å¿ƒæŠ€æœ¯æ ‡ç­¾2", "è¡Œä¸šæ ‡ç­¾"],
      "importance": 9
    }}
    ---END_METADATA---
    
    ---START_CONTENT---
    # ğŸ’¡ é¦–å¸­æ´å¯Ÿ (Chief Insight)
    (ç”¨ä¸€æ®µè¯åˆæˆä»Šæ—¥çš„æ•´ä½“å±€åŠ¿ã€‚å¿…é¡»åŸºäºçœŸå®å‘ç”Ÿçš„äº‹ä»¶ï¼Œé¿å…ç©ºæ³›è¯„è®º)
    
    ## ğŸ”¥ æ ¸å¿ƒæƒ…æŠ¥ (4-6æ¡)
    
    ### 1. [æƒ…æŠ¥æ ‡é¢˜]
    **æ¥æº**: [åª’ä½“/ç¤¾åŒºåç§°](URL) | å‘å¸ƒæ—¶é—´
    
    - **æ·±åº¦æ‹†è§£**: (50-100å­—ï¼Œæ ¸å¿ƒæŠ€æœ¯æˆ–äº‹ä»¶è„‰ç»œ)
    
    - **ä¸ºä½•é‡è¦**: (ä¸€å¥è¯ç‚¹å‡ºè¯´æ˜å¯¹è¡Œä¸šçš„çŸ­æœŸå’Œé•¿æœŸå½±å“ã€‚)
    
    - **ç¤¾åŒºå£°éŸ³**: (Redditã€Hacker Newsã€Twitter ä¸Šçš„å…³é”®äº‰è®®æˆ–å¥½è¯„)
    
    ### 2. [æƒ…æŠ¥æ ‡é¢˜]
    ...
    
    ### 3. [æƒ…æŠ¥æ ‡é¢˜]
    ...
    
    ## ğŸ› ï¸ æå®¢æ¨è (GitHub/Tools)
    - **[é¡¹ç›®å](URL)**: (ä¸€å¥è¯ä»‹ç»æ ¸å¿ƒåŠŸèƒ½ + ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ï¼ˆå¦‚ï¼šStar æ•°å¢é•¿ã€è§£å†³çš„ç—›ç‚¹ç­‰ï¼‰)
    
    ## ğŸ”— åŸå§‹æƒ…æŠ¥æ¥æº
    - [æ ‡é¢˜](URL)
    ---END_CONTENT---
    """

    response = client.models.generate_content(
        model='gemini-3-pro-preview',  # ä½¿ç”¨ä¸åŸç‰ˆç›¸åŒçš„æ¨¡å‹
        contents=prompt,
        config=types.GenerateContentConfig(
            tools=[types.Tool(google_search=types.GoogleSearch())],
            thinking_config=types.ThinkingConfig(include_thoughts=True)  # å¯ç”¨æ€è€ƒæ¨¡å¼
        )
    )
    return response.text

def parse_gemini_response(raw_text):
    """ä»åŸå§‹æ–‡æœ¬ä¸­æå–å…ƒæ•°æ®å’Œæ­£æ–‡å†…å®¹"""
    try:
        metadata_match = re.search(r'---START_METADATA---(.*?)---END_METADATA---', raw_text, re.DOTALL)
        content_match = re.search(r'---START_CONTENT---(.*?)---END_CONTENT---', raw_text, re.DOTALL)
        
        if metadata_match and content_match:
            json_str = metadata_match.group(1).strip()
            # æ¸…ç† JSON ä¸­å¯èƒ½åŒ…è£¹çš„ markdown ä»£ç å—æ ‡è¯†
            json_str = re.sub(r'^```json|```$', '', json_str, flags=re.MULTILINE).strip()
            metadata = json.loads(json_str)
            content = content_match.group(1).strip()
            return metadata, content
        else:
            return None, raw_text
    except Exception as e:
        print(f"âš ï¸ è§£æè§£æå‡ºé”™: {e}")
        return None, raw_text

def split_content_to_blocks(text):
    """
    å°† Markdown æ–‡æœ¬è½¬æ¢ä¸º Notion çš„ç»“æ„åŒ– Block
    æ”¯æŒï¼šHeading 1/2/3, Bullet List, Quote, Paragraph
    """
    blocks = []
    lines = text.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line: 
            continue
        
        # 1. Heading 1 (# )
        if line.startswith('# '):
            content = line[2:].strip()[:2000]
            blocks.append({
                "object": "block", "type": "heading_1",
                "heading_1": {"rich_text": [{"text": {"content": content}}]}
            })
        # 2. Heading 2 (## )
        elif line.startswith('## '):
            content = line[3:].strip()[:2000]
            blocks.append({
                "object": "block", "type": "heading_2",
                "heading_2": {"rich_text": [{"text": {"content": content}}]}
            })
        # 3. Heading 3 (### )
        elif line.startswith('### '):
            content = line[4:].strip()[:2000]
            blocks.append({
                "object": "block", "type": "heading_3",
                "heading_3": {"rich_text": [{"text": {"content": content}}]}
            })
        # 4. Bullet List (- or * )
        elif line.startswith('- ') or line.startswith('* '):
            content = line[2:].strip()[:2000]
            blocks.append({
                "object": "block", "type": "bulleted_list_item",
                "bulleted_list_item": {"rich_text": [{"text": {"content": content}}]}
            })
        # 5. Quote (> )
        elif line.startswith('> '):
            content = line[2:].strip()[:2000]
            blocks.append({
                "object": "block", "type": "quote",
                "quote": {"rich_text": [{"text": {"content": content}}]}
            })
        # 6. Default Paragraph
        else:
            # Notion block character limit is 2000
            content = line[:2000]
            blocks.append({
                "object": "block", "type": "paragraph",
                "paragraph": {"rich_text": [{"text": {"content": content}}]}
            })
            
    return blocks

def save_to_notion(metadata, content):
    """å°†å·²è§£æçš„å†…å®¹åŒæ­¥åˆ° Notion"""
    print("ğŸ““ æ­£åœ¨åŒæ­¥è‡³ Notion...")
    try:
        # è·å–å½“å‰æ—¥æœŸï¼ˆåŒ—äº¬æ—¶é—´ï¼‰
        publish_date = datetime.now(TZ_CN).strftime('%Y-%m-%d')
        
        body_blocks = split_content_to_blocks(content)
        
        # æ„å»º propertiesï¼ŒåŒ…æ‹¬å‘å¸ƒæ—¥æœŸ
        properties = {
            "æ ‡é¢˜": {"title": [{"text": {"content": metadata.get('title', 'ä»Šæ—¥ AI ç®€æŠ¥')}}]},
            "ä¸€å¥è¯æ‘˜è¦": {"rich_text": [{"text": {"content": metadata.get('summary', '')}}]},
            "æ ¸å¿ƒé¢†åŸŸ": {"multi_select": [{"name": tag} for tag in metadata.get('tags', []) if tag]},
            "å‘å¸ƒæ—¥æœŸ": {"date": {"start": publish_date}}
        }
        
        notion.pages.create(
            parent={"database_id": DATABASE_ID},
            properties=properties,
            children=body_blocks[:100] 
        )
        print(f"âœ… Notion åŒæ­¥æˆåŠŸï¼å‘å¸ƒæ—¥æœŸ: {publish_date}")
    except Exception as e:
        print(f"âŒ Notion ä¿å­˜å¤±è´¥: {e}")

def update_archive_index(archive_dir):
    """éå†æ–‡ä»¶å¤¹ï¼Œç”Ÿæˆå½’æ¡£åˆ—è¡¨"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not os.path.exists(archive_dir):
        print(f"âš ï¸ ç›®å½• {archive_dir} ä¸å­˜åœ¨ï¼Œè·³è¿‡ç´¢å¼•æ›´æ–°")
        return

    files = [f for f in os.listdir(archive_dir) if f.endswith(".md") and f != "index.md"]
    files.sort(reverse=True) # æŒ‰æ—¥æœŸå€’åºæ’
    
    index_path = os.path.join(archive_dir, "index.md")
    with open(index_path, "w", encoding="utf-8") as f:
        f.write("# ğŸ“… æ—¥æŠ¥å½’æ¡£\n\n")
        f.write("ç‚¹å‡»ä¸‹æ–¹æ—¥æœŸæŸ¥çœ‹å½“å¤©çš„ AI æ·±åº¦ç ”ç©¶ï¼š\n\n")
        for file in files:
            date_name = file.replace(".md", "")
            f.write(f"- [{date_name} çš„ AI ç®€æŠ¥]({file})\n")
    print(f"âœ… å½’æ¡£ç´¢å¼•å·²åŒæ­¥æ›´æ–°è‡³: {index_path}")

def update_homepage(metadata, content):
    """åŠ¨æ€æ›´æ–°é¦–é¡µ index.mdï¼Œå±•ç¤ºæœ€æ–°ç®€æŠ¥é¢„è§ˆ"""
    print("ğŸ  æ­£åœ¨æ›´æ–°é¦–é¡µåŠ¨æ€å†…å®¹...")
    date_str = datetime.now(TZ_CN).strftime('%Y-%m-%d')
    
    # 1. æ„é€ é¦–é¡µå†…å®¹
    # æˆ‘ä»¬åªå–æ­£æ–‡çš„å‰ 600 ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆï¼Œé¿å…é¦–é¡µè¿‡é•¿
    preview_content = content[:600] + "..." if len(content) > 600 else content
    
    homepage_template = f"""# ğŸ¤– AI æ¯æ—¥æ·±åº¦ç ”ç©¶ç®€æŠ¥

> **æœ€æ–°åŠ¨æ€ ({date_str})**: {metadata.get('summary')}

## ğŸŒŸ ä»Šæ—¥å¤´æ¡: {metadata.get('title')}

{preview_content}

---

### ğŸ”— å¿«é€Ÿé“¾æ¥
- [ğŸ“… æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š](archives/{date_str}.md)
- [ğŸ“š å¾€æœŸå†…å®¹å½’æ¡£](archives/index.md)

### ğŸ› ï¸ è®¢é˜…è¯´æ˜
æœ¬ç«™ç‚¹ç”± **Gemini 3 Pro** é©±åŠ¨ï¼Œæ¯æ—¥æ—© 8 ç‚¹é€šè¿‡ **GitHub Actions** è‡ªåŠ¨æ·±åº¦æœç´¢å…¨ç½‘ AI æƒ…æŠ¥å¹¶æ›´æ–°ã€‚

<iframe data-tally-src="https://tally.so/embed/kd9P9J?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="200" frameborder="0" marginheight="0" marginwidth="0" title="subscribe"></iframe>
<script>var d=document,w="https://tally.so/widgets/embed.js",v=function(){{"undefined"!=typeof Tally?Tally.loadEmbeds():d.querySelectorAll("iframe[data-tally-src]:not([src])").forEach((function(e){{e.src=e.dataset.tallySrc}}))}};if("undefined"!=typeof Tally)v();else if(d.querySelector('script[src="'+w+'"]')==null){{var s=d.createElement("script");s.src=w,s.onload=v,s.onerror=v,d.body.appendChild(s);}}</script>

---
*ä¸Šæ¬¡æ›´æ–°æ—¶é—´ï¼š{datetime.now(TZ_CN).strftime('%Y-%m-%d %H:%M:%S')}*
"""

    # 2. å†™å…¥ docs/index.md
    with open("docs/index.md", "w", encoding="utf-8") as f:
        f.write(homepage_template)
    print("âœ… é¦–é¡µå·²æ›´æ–°ä¸ºæœ€æ–°å†…å®¹")


def save_to_markdown_file(metadata, content):
    """å°†å†…å®¹ä¿å­˜ä¸º Markdown æ–‡ä»¶ï¼Œä¾› MkDocs ä½¿ç”¨"""
    date_str = datetime.now(TZ_CN).strftime('%Y-%m-%d')
    print(f"ğŸŒ æ­£åœ¨ç”Ÿæˆç½‘é¡µæ–‡ä»¶: {date_str}.md")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    save_dir = "docs/archives"
    os.makedirs(save_dir, exist_ok=True)
    file_path = f"{save_dir}/{date_str}.md"
    
    # æ„é€ å¸¦æœ‰ Front Matter çš„å†…å®¹ï¼Œè¿™æœ‰åˆ©äº MkDocs çš„ SEO å’Œé¡µé¢æ˜¾ç¤º
    full_markdown = f"""---
title: {metadata.get('title')}
date: {date_str}
tags: {metadata.get('tags', [])}
description: {metadata.get('summary')}
---

# {metadata.get('title')}

> **æ‘˜è¦**: {metadata.get('summary')}

{content}

<div class="subscribe-card">
    <div class="subscribe-title">ğŸ“© è®¢é˜…æ¯æ—¥ AI ç®€æŠ¥</div>
    <div class="subscribe-desc">æ¯å¤©æ—©æ™¨ï¼Œå°†æœ€æ–°çš„ AI çªç ´ä¸æ·±åº¦æ´å¯Ÿç›´æ¥å‘é€åˆ°æ‚¨çš„æ”¶ä»¶ç®±ã€‚</div>
<iframe data-tally-src="https://tally.so/embed/kd9P9J?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="200" frameborder="0" marginheight="0" marginwidth="0" title="subscribe"></iframe>
<script>var d=document,w="https://tally.so/widgets/embed.js",v=function(){{"undefined"!=typeof Tally?Tally.loadEmbeds():d.querySelectorAll("iframe[data-tally-src]:not([src])").forEach((function(e){{e.src=e.dataset.tallySrc}}))}};if("undefined"!=typeof Tally)v();else if(d.querySelector('script[src="'+w+'"]')==null){{var s=d.createElement("script");s.src=w,s.onload=v,s.onerror=v,d.body.appendChild(s);}}</script>
    <div style="margin-top: 10px; font-size: 0.8em; opacity: 0.7;">æˆ–è€…å›å¤ GitHub Issue è¿›è¡Œè¯„è®ºäº’åŠ¨</div>
</div>

---
*ç”Ÿæˆæ—¶é—´ï¼š{datetime.now(TZ_CN).strftime('%Y-%m-%d %H:%M:%S')}*
"""

    with open(file_path, "w", encoding="utf-8") as f:
        f.write(full_markdown)
    print(f"âœ… ç½‘é¡µæ–‡ä»¶å·²ä¿å­˜è‡³: {file_path}")

def publish_to_github_issue(metadata, content):
    """å°†ç®€æŠ¥å‘å¸ƒä¸ºæ‚¨ GitHub ä»“åº“çš„ Issueï¼Œå®ç°é‚®ä»¶æ¨é€è®¢é˜…"""
    if not GITHUB_TOKEN or not GITHUB_REPO:
        print("âš ï¸ æœªé…ç½® GITHUB_TOKEN æˆ– GITHUB_REPOSITORYï¼Œè·³è¿‡ Issue å‘å¸ƒ")
        return

    print("ğŸ“§ æ­£åœ¨å‘å¸ƒ GitHub Issue...")
    try:
        g = Github(GITHUB_TOKEN)
        repo = g.get_repo(GITHUB_REPO)
        
        # æ„é€  Issue æ ‡é¢˜å’Œæ­£æ–‡
        date_str = datetime.now(TZ_CN).strftime('%Y-%m-%d')
        issue_title = f"{date_str} | {metadata.get('title')}"
        
        # åœ¨æ­£æ–‡é¡¶éƒ¨åŠ ä¸ŠåŸæ–‡é“¾æ¥ï¼Œå¢åŠ å¯¼æµ
        issue_body = f"""
> **æ‘˜è¦**: {metadata.get('summary')}

[ğŸ‘‰ ç‚¹å‡»æŸ¥çœ‹å®Œæ•´æ’ç‰ˆæŠ¥å‘Š](https://cxyac.github.io/AI-Daily-News-Brief/archives/{date_str}/)

---

{content}

---
*æœ¬æŠ¥å‘Šç”± AI Agent è‡ªåŠ¨ç”Ÿæˆï¼Œå›å¤æœ¬ Issue å¯å‚ä¸è®¨è®ºã€‚*
"""
        repo.create_issue(title=issue_title, body=issue_body, labels=["daily-brief"])
        print(f"âœ… GitHub Issue å·²å‘å¸ƒï¼š{issue_title}")
    except Exception as e:
        print(f"âŒ GitHub Issue å‘å¸ƒå¤±è´¥: {e}")

if __name__ == "__main__":
    if not GEMINI_API_KEY:
        print("âŒ é”™è¯¯: GEMINI_API_KEY æœªè®¾ç½®")
        exit(1)

    print("="*70)
    print("ğŸ¤– AI Daily News Brief - Deep Research Edition")
    print("="*70)

    # 1. è¿è¡Œ Deep Researchï¼ˆå¸¦é™çº§æœºåˆ¶ï¼‰
    try:
        raw_report = run_deep_research()
        print("\nâœ… Deep Research æ‰§è¡ŒæˆåŠŸ")
    except Exception as e:
        print(f"\nâš ï¸ Deep Research å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ: {e}")
        try:
            raw_report = run_gemini3_research_fallback()
            print("\nâœ… é™çº§æ–¹æ¡ˆæ‰§è¡ŒæˆåŠŸ")
        except Exception as e2:
            print(f"\nâŒ é™çº§æ–¹æ¡ˆä¹Ÿå¤±è´¥äº†: {e2}")
            exit(1)
    
    # 2. è§£æå†…å®¹
    meta, body = parse_gemini_response(raw_report)
    if not meta:
        meta = {"title": f"AI æ·±åº¦ç®€æŠ¥ - {datetime.now(TZ_CN).strftime('%Y-%m-%d')}", "summary": "ä»Šæ—¥æƒ…æŠ¥å·²é€è¾¾", "tags": ["AI"]}

    print(f"\nğŸ“‹ æŠ¥å‘Šæ ‡é¢˜: {meta.get('title')}")
    print(f"ğŸ“Š æŠ¥å‘Šé•¿åº¦: {len(body)} å­—ç¬¦")

    # 3. å­˜å‚¨å½“å¤©è¯¦æƒ…é¡µ (.md æ–‡ä»¶)
    save_to_markdown_file(meta, body) 

    # 4. æ›´æ–°"ç´¢å¼•ç›®å½•" (è®© Archives é¡µé¢å‡ºç°æ–°é“¾æ¥)
    update_archive_index("docs/archives") 

    # 5. æ›´æ–°"ç½‘ç«™é¦–é¡µ" (è®©é¦–é¡µå±•ç¤ºä»Šå¤©çš„é¢„è§ˆ)
    update_homepage(meta, body)

    # 6. åŒæ­¥ Notion (å¯é€‰å¤‡ä»½)
    if NOTION_TOKEN and DATABASE_ID:
        save_to_notion(meta, body)

    # 7. å‘å¸ƒ GitHub Issue (ä½œä¸ºé‚®ä»¶è®¢é˜…æ¸ é“)
    publish_to_github_issue(meta, body)

    # 8. SMTP é‚®ä»¶æ¨é€
    send_email_newsletter(meta, body)

    print("\n" + "="*70)
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("="*70)